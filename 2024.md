## 2024

### 18.11.2024
- **FNN:** weights -> strength of the connections between neurons. 

- weights are learned during the training process using backpropagation.

- sigmoid function, hyperbolic tangent function, rectified linear unit (ReLU) function

---

### 22.11.2024
- **Linear Layer:** in a neural network is a fundamental building block that performs a linear transformation on its input. 

- It is commonly used in fully connected (dense) layers and is defined mathematically as: y = W x + b

- Compute Gradients (Backpropagation)

- Loss function quantifies the error between predicted and actual values.

- Update the weights using an optimization algorithm.

---

### 25.11.2024
- **Activation Function on TNN:** no activation function = simple linear model (linear regression)

- Stacking multiple linear layers would still result in a linear transformation.

- Functions like sigmoid and softmax allow outputs to be interpreted as probabilities.

- **Common Activation Functions and Use Cases:**
    - Sigmoid: For binary classification tasks.
    - ReLU: Default choice for hidden layers in deep networks.
    - Tanh: Scales outputs to [-1, 1], useful for zero-centered data.
    - Softmax: Converts outputs to probs for multi-class classification.

- CrossEntropyLoss = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)
    - No softmax in last layer.
